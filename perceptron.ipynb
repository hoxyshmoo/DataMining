{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"Torgersen\" at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\afamy\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2315\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"Torgersen\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\afamy\\Documents\\GitHub\\DataMining\\perceptron.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/afamy/Documents/GitHub/DataMining/perceptron.ipynb#W0sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m datatrain\u001b[39m.\u001b[39mloc[datatrain[\u001b[39m'\u001b[39m\u001b[39mspecies\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mGentoo\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mspecies\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/afamy/Documents/GitHub/DataMining/perceptron.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m datatrain\u001b[39m.\u001b[39mloc[datatrain[\u001b[39m'\u001b[39m\u001b[39mspecies\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mChinstrap\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mspecies\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/afamy/Documents/GitHub/DataMining/perceptron.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m datatrain \u001b[39m=\u001b[39m datatrain\u001b[39m.\u001b[39;49mapply(pd\u001b[39m.\u001b[39;49mto_numeric)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/afamy/Documents/GitHub/DataMining/perceptron.ipynb#W0sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#change dataframe to array\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/afamy/Documents/GitHub/DataMining/perceptron.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m datatrain_array \u001b[39m=\u001b[39m datatrain\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\afamy\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8839\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8828\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   8830\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   8831\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   8832\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8837\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   8838\u001b[0m )\n\u001b[1;32m-> 8839\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\afamy\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    725\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\afamy\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    853\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\afamy\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    865\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    866\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    868\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    869\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    870\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    871\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\afamy\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\numeric.py:184\u001b[0m, in \u001b[0;36mto_numeric\u001b[1;34m(arg, errors, downcast)\u001b[0m\n\u001b[0;32m    182\u001b[0m coerce_numeric \u001b[39m=\u001b[39m errors \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    183\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     values, _ \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmaybe_convert_numeric(\n\u001b[0;32m    185\u001b[0m         values, \u001b[39mset\u001b[39;49m(), coerce_numeric\u001b[39m=\u001b[39;49mcoerce_numeric\n\u001b[0;32m    186\u001b[0m     )\n\u001b[0;32m    187\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\afamy\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2357\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to parse string \"Torgersen\" at position 0"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SECTION 1 : Load and setup data for training\n",
    "the datasets separated in two files from originai datasets:\n",
    "penguin-clean-train.csv = datasets for training purpose, 70% from the original data\n",
    "penguin-clean-test.csv  = datasets for testing purpose, 30% from the original data\n",
    "\"\"\"\n",
    "\n",
    "#Section 1.1 Data Loading\n",
    "import pandas as pd\n",
    "\n",
    "#load\n",
    "\n",
    "datatrain = pd.read_csv(\"penguinsFeatures.csv\")\n",
    "\n",
    "\n",
    "# data.info()\n",
    "# datatrain = pd.read_csv('../Datasets/penguins-clean-train.csv')\n",
    "\n",
    "#Section 1.2 Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#change string value to numeric\n",
    "datatrain.loc[datatrain['species']=='Adelie', 'species']=0\n",
    "datatrain.loc[datatrain['species']=='Gentoo', 'species']=1\n",
    "datatrain.loc[datatrain['species']=='Chinstrap', 'species']=2\n",
    "datatrain = datatrain.apply(pd.to_numeric)\n",
    "\n",
    "#change dataframe to array\n",
    "datatrain_array = datatrain.values\n",
    "\n",
    "#split x and y (feature and target)\n",
    "xtrain = datatrain_array[:,1:]\n",
    "ytrain = datatrain_array[:,0]\n",
    "\n",
    "#standardize\n",
    "#palmer-penguin dataset has varying scales\n",
    "scaler = StandardScaler()\n",
    "xtrain = scaler.fit_transform(xtrain)\n",
    "\n",
    "\"\"\"\n",
    "SECTION 2 : Build and Train Model\n",
    "Multilayer perceptron model, with one hidden layer.\n",
    "input layer : 4 neuron, represents the feature from Palmer Penguin dataset\n",
    "hidden layer : 10 neuron, activation using ReLU\n",
    "output layer : 3 neuron, represents the number of species, Softmax Layer\n",
    "optimizer = stochastic gradient descent with no batch-size\n",
    "loss function = categorical cross entropy\n",
    "learning rate = default from keras.optimizer.SGD, 0.001\n",
    "epoch = 50\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#change target format\n",
    "ytrain = to_categorical(ytrain) \n",
    "\n",
    "#build model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(10, input_shape=(4,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "#choose optimizer and loss function\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "#train\n",
    "model.fit(xtrain, ytrain, epochs=50, batch_size=16)\n",
    "\n",
    "\"\"\"\n",
    "SECTION 3 : Testing model\n",
    "\"\"\"\n",
    "#load\n",
    "datatest = pd.read_csv('../Datasets/penguins-clean-test.csv')\n",
    "\n",
    "#change string value to numeric\n",
    "datatest.loc[datatest['species']=='Adelie', 'species']=0\n",
    "datatest.loc[datatest['species']=='Gentoo', 'species']=1\n",
    "datatest.loc[datatest['species']=='Chinstrap', 'species']=2\n",
    "datatest = datatest.apply(pd.to_numeric)\n",
    "\n",
    "#change dataframe to array\n",
    "datatest_array = datatest.values\n",
    "\n",
    "#split x and y (feature and target)\n",
    "xtest = datatest_array[:,1:]\n",
    "ytest = datatest_array[:,0]\n",
    "\n",
    "#standardization \n",
    "xtest = scaler.transform(xtest)\n",
    "\n",
    "#get prediction\n",
    "classes = np.argmax(model.predict(xtest), axis=-1)\n",
    "\n",
    "#get accuration\n",
    "import numpy as np\n",
    "accuration = np.sum(classes == ytest)/len(ytest) * 100\n",
    "\n",
    "print(\"Test Accuration : \" + str(accuration) + '%')\n",
    "print(\"Prediction :\")\n",
    "print(classes)\n",
    "print(\"Target :\")\n",
    "print(np.asarray(ytest,dtype=\"int32\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d7ced7024180f85191dadd1b1601e0aff182c96dfba1c12d901eda4996d8099"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
